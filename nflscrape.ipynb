{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b8f006e-4d41-4da2-b738-edeb7bec7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: transfer nfl web scraping programs here. Clean and condense the code.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b003dadf-29aa-469e-860c-f00f55769713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arizona-cardinals', 'baltimore-ravens', 'atlanta-falcons', 'buffalo-bills', 'carolina-panthers', 'cincinnati-bengals', 'chicago-bears', 'cleveland-browns', 'dallas-cowboys', 'denver-broncos', 'detroit-lions', 'houston-texans', 'green-bay-packers', 'indianapolis-colts', 'los-angeles-rams', 'jacksonville-jaguars', 'minnesota-vikings', 'kansas-city-chiefs', 'new-orleans-saints', 'las-vegas-raiders', 'new-york-giants', 'los-angeles-chargers', 'philadelphia-eagles', 'miami-dolphins', 'san-francisco-49ers', 'new-england-patriots', 'seattle-seahawks', 'new-york-jets', 'tampa-bay-buccaneers', 'pittsburgh-steelers', 'washington-commanders', 'tennessee-titans']\n"
     ]
    }
   ],
   "source": [
    "team_list = ['arizona-cardinals',\n",
    "'baltimore-ravens',\n",
    "'atlanta-falcons',\n",
    "'buffalo-bills',\n",
    "'carolina-panthers',\n",
    "'cincinnati-bengals',\n",
    "'chicago-bears',\n",
    "'cleveland-browns',\n",
    "'dallas-cowboys',\n",
    "'denver-broncos',\n",
    "'detroit-lions',\n",
    "'houston-texans',\n",
    "'green-bay-packers',\n",
    "'indianapolis-colts',\n",
    "'los-angeles-rams',\n",
    "'jacksonville-jaguars',\n",
    "'minnesota-vikings',\n",
    "'kansas-city-chiefs',\n",
    "'new-orleans-saints',\n",
    "'las-vegas-raiders',\n",
    "'new-york-giants',\n",
    "'los-angeles-chargers',\n",
    "'philadelphia-eagles',\n",
    "'miami-dolphins',\n",
    "'san-francisco-49ers',\n",
    "'new-england-patriots',\n",
    "'seattle-seahawks',\n",
    "'new-york-jets',\n",
    "'tampa-bay-buccaneers',\n",
    "'pittsburgh-steelers',\n",
    "'washington-commanders',\n",
    "'tennessee-titans',]\n",
    "\n",
    "print(team_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "058b22c6-775d-4881-8239-38a4f9fee1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arizona-cardinals\n",
      "1/32\n",
      "Scraping roster table for https://www.nfl.com/teams/arizona-cardinals/roster/\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m     row_data \u001b[38;5;241m=\u001b[39m [td\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m td \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m     42\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df)\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m row_data\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# TODO: \u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Working on building a second DF that will be merged with the NFL.com table scrape.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Need to add columns for the Player Name schema, Team, and Year.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfindAll(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfl-o-roster__player-name nfl-o-cta--link\u001b[39m\u001b[38;5;124m'\u001b[39m}):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    817\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 818\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1785\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1782\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m-> 1785\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1786\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1789\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:2160\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value):\n\u001b[0;32m   2158\u001b[0m         \u001b[38;5;66;03m# must have conforming columns\u001b[39;00m\n\u001b[0;32m   2159\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[1;32m-> 2160\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot set a row with mismatched columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2162\u001b[0m     value \u001b[38;5;241m=\u001b[39m Series(value, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mindexer)\n\u001b[0;32m   2164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj):\n\u001b[0;32m   2165\u001b[0m     \u001b[38;5;66;03m# We will ignore the existing dtypes instead of using\u001b[39;00m\n\u001b[0;32m   2166\u001b[0m     \u001b[38;5;66;03m#  internals.concat logic\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "# Send any Errors for specific team choice to a list:\n",
    "exceptions = []\n",
    "\n",
    "# Player links list is global to allow for appending in loop farther below.\n",
    "Player_links = []\n",
    "\n",
    "\n",
    "# loop through NFL.com team rosters and pull out all player names, positions, etc.\n",
    "for i in range(0, len(team_list)):\n",
    "    try:\n",
    "        team_choice = team_list[i]\n",
    "        i += 1\n",
    "        print(team_choice)\n",
    "        print(str(i) + '/' + str(len(team_list)))\n",
    "\n",
    "        root = 'https://www.nfl.com/teams/'\n",
    "        url_end_piece = '/roster/'\n",
    "\n",
    "        url = root + team_choice + url_end_piece\n",
    "        print('Scraping roster table for '+ url)\n",
    "\n",
    "\n",
    "        # create the soup object here\n",
    "        # this is where you input the url and output the soup object that parses the webpage\n",
    "        result = requests.get(url)\n",
    "        webpage = result.content\n",
    "        soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "\n",
    "        # find the html table mark up and then find all row tag objects inside that table object\n",
    "        # nfl.com stats pages only contain one table, so don't need to worry about processing multiple tables in this web scrape\n",
    "        table = soup.find('table', {'summary': 'Roster'})\n",
    "\n",
    "        headers = []\n",
    "        for i in table.find_all('th'):\n",
    "            title = i.text.strip()\n",
    "            headers.append(title)\n",
    "\n",
    "        df = pd.DataFrame(columns = headers)\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            data = row.findAll('td')\n",
    "            row_data = [td.text.strip() for td in data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row_data\n",
    "           \n",
    "        \n",
    "        # TODO: \n",
    "        # Working on building a second DF that will be merged with the NFL.com table scrape.\n",
    "        # Need to add columns for the Player Name schema, Team, and Year.\n",
    "       \n",
    "        for link in soup.findAll('a', {'class': 'nfl-o-roster__player-name nfl-o-cta--link'}):\n",
    "            Player_links.append(link.get('href'))\n",
    "        # print(Player_links)\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "# Try adding the player webpage link as a row item:      \n",
    "        # playerlink_df_headers = ['Player_Link', 'Team Name', 'Year']  \n",
    "        # playerlink_df = pd.DataFrame(columns = playerlink_df_headers)\n",
    "        # df['Player_link'] = str(soup.find('tr', {'nfl-o-roster__player-name nfl-o-cta--link'}))\n",
    "        # df['Team Name'] = str(soup.find('div', {'nfl-c-team-header__title'}).text.strip())\n",
    "        # df['Year'] = date.today().year\n",
    "        # print(playerlink_df)\n",
    "             \n",
    "# Then append the dataframes\n",
    "      \n",
    "            \n",
    "    except AttributeError:    # the except statement sends Attribute errors to a list that is exported after the loop is finished\n",
    "        exceptions.append(team_choice)\n",
    "\n",
    "    with open(r'C:\\Users\\EvanS\\Programming\\PyCharm\\Projects\\NFL-Web-Scrape-V2\\Scrap files\\review_roster_scrape_output.csv', 'w') as fp:\n",
    "        fp.write('\\n'.join(exceptions))\n",
    "        \n",
    "\n",
    "Player_link_df  = pd.DataFrame(data = Player_links, columns = ['Player link'])\n",
    "print(Player_link_df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a540e92-dbcc-4737-932c-d4a474bbcc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0002b2b9-e29f-44be-b9d8-ae7d6b4e8aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfde5db-e5f5-4c2f-b843-842f14ab7e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061f6c4-e470-4a83-a47f-c12873ce825f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
