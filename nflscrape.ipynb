{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b8f006e-4d41-4da2-b738-edeb7bec7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: transfer nfl web scraping programs here. Clean and condense the code.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b003dadf-29aa-469e-860c-f00f55769713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arizona-cardinals', 'baltimore-ravens', 'atlanta-falcons', 'buffalo-bills', 'carolina-panthers', 'cincinnati-bengals', 'chicago-bears', 'cleveland-browns', 'dallas-cowboys', 'denver-broncos', 'detroit-lions', 'houston-texans', 'green-bay-packers', 'indianapolis-colts', 'los-angeles-rams', 'jacksonville-jaguars', 'minnesota-vikings', 'kansas-city-chiefs', 'new-orleans-saints', 'las-vegas-raiders', 'new-york-giants', 'los-angeles-chargers', 'philadelphia-eagles', 'miami-dolphins', 'san-francisco-49ers', 'new-england-patriots', 'seattle-seahawks', 'new-york-jets', 'tampa-bay-buccaneers', 'pittsburgh-steelers', 'washington-commanders', 'tennessee-titans']\n"
     ]
    }
   ],
   "source": [
    "team_list = ['arizona-cardinals',\n",
    "'baltimore-ravens',\n",
    "'atlanta-falcons',\n",
    "'buffalo-bills',\n",
    "'carolina-panthers',\n",
    "'cincinnati-bengals',\n",
    "'chicago-bears',\n",
    "'cleveland-browns',\n",
    "'dallas-cowboys',\n",
    "'denver-broncos',\n",
    "'detroit-lions',\n",
    "'houston-texans',\n",
    "'green-bay-packers',\n",
    "'indianapolis-colts',\n",
    "'los-angeles-rams',\n",
    "'jacksonville-jaguars',\n",
    "'minnesota-vikings',\n",
    "'kansas-city-chiefs',\n",
    "'new-orleans-saints',\n",
    "'las-vegas-raiders',\n",
    "'new-york-giants',\n",
    "'los-angeles-chargers',\n",
    "'philadelphia-eagles',\n",
    "'miami-dolphins',\n",
    "'san-francisco-49ers',\n",
    "'new-england-patriots',\n",
    "'seattle-seahawks',\n",
    "'new-york-jets',\n",
    "'tampa-bay-buccaneers',\n",
    "'pittsburgh-steelers',\n",
    "'washington-commanders',\n",
    "'tennessee-titans',]\n",
    "\n",
    "print(team_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "058b22c6-775d-4881-8239-38a4f9fee1c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3587308834.py, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[39], line 45\u001b[1;36m\u001b[0m\n\u001b[1;33m    i = i +1\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "### Send any Errors for specific team choice to a list:\n",
    "exceptions = []\n",
    "\n",
    "### Player links list is global to allow for appending this list in loop farther below.\n",
    "Player_links = []\n",
    "\n",
    "\n",
    "### loop through NFL.com team rosters and pull out all player names, positions, etc.\n",
    "for i in range(0, 3):  #len(team_list)):\n",
    "    try:\n",
    "        team_choice = team_list[i]\n",
    "        print(team_choice)\n",
    "        print(str(i) + '/' + str(len(team_list)))\n",
    "\n",
    "        root = 'https://www.nfl.com/teams/'\n",
    "        url_end_piece = '/roster/'\n",
    "\n",
    "        url = root + team_choice + url_end_piece\n",
    "        print('Scraping roster table for '+ url)\n",
    "\n",
    "\n",
    "        ### create the soup object here\n",
    "        ### this is where you input the url and output the soup object that parses the webpage\n",
    "        result = requests.get(url)\n",
    "        webpage = result.content\n",
    "        soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "\n",
    "        ### find the html table mark up and then find all row tag objects inside that table object\n",
    "        ### nfl.com stats pages only contain one table, so don't need to worry about processing multiple tables in this web scrape\n",
    "        table = soup.find('table', {'summary': 'Roster'})\n",
    "\n",
    "        headers = []\n",
    "        for i in table.find_all('th'):\n",
    "            title = i.text.strip()\n",
    "            headers.append(title)\n",
    "            #headers.append(['Player link'])\n",
    "\n",
    "        df = pd.DataFrame(columns = headers)\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            data = row.findAll('td')\n",
    "            row_data = [td.text.strip() for td in data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row_data\n",
    "            #print(df)\n",
    "            \n",
    "           \n",
    "        \n",
    "        ### TODO: \n",
    "        ### Working on building a second DF that will be merged with the NFL.com table scrape???\n",
    "        ### or try to add the additional column and row data in the loop above.\n",
    "        ### Need to add columns for the Player Name schema, Team, and Year.\n",
    "       \n",
    "        # for link in soup.findAll('a', {'class': 'nfl-o-roster__player-name nfl-o-cta--link'}):\n",
    "        #     Player_links.append(link.get('href'))\n",
    "        # print(Player_links)\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "### Try adding the player webpage link as a row item:      \n",
    "        # playerlink_df_headers = ['Player_Link', 'Team Name', 'Year']  \n",
    "        # playerlink_df = pd.DataFrame(columns = playerlink_df_headers)\n",
    "        # df['Player_link'] = str(soup.find('tr', {'nfl-o-roster__player-name nfl-o-cta--link'}))\n",
    "        # df['Team Name'] = str(soup.find('div', {'nfl-c-team-header__title'}).text.strip())\n",
    "        # df['Year'] = date.today().year\n",
    "        # print(playerlink_df)\n",
    "             \n",
    "### Then append the dataframes\n",
    "      \n",
    "            \n",
    "    except AttributeError:    # the except statement sends Attribute errors to a list that is exported after the loop is finished\n",
    "        exceptions.append(team_choice)\n",
    "\n",
    "    with open(r'C:\\Users\\EvanS\\Programming\\PyCharm\\Projects\\NFL-Web-Scrape-V2\\Scrap files\\review_roster_scrape_output.csv', 'w') as fp:\n",
    "        fp.write('\\n'.join(exceptions))\n",
    "        \n",
    "### Manually put together second dataframe with only the Player link field:\n",
    "# Player_link_df  = pd.DataFrame(data = Player_links, columns = ['Player link'])\n",
    "# print(Player_link_df)\n",
    "\n",
    "\n",
    "# #print(df)\n",
    "df.to_csv(r'C:\\Users\\EvanS\\Programming\\PyCharm\\Projects\\NFL-Web-Scrape-V2\\Scrap files\\test.csv')\n",
    "\n",
    "### The dataframe is being overwritten with each loop pass. Need to fix it so all pages scraped go to the same dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4093448-4443-4338-9c60-a3c138dac1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\EvanS\\\\Programming\\\\PyCharm\\\\Projects\\\\NFL-Web-Scrape-V2'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50431da7-ccc4-4ee3-a6a8-cde433c5d8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8e116-2ed1-47f0-849f-1f215b547b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce4d4b8-8303-46f7-bddd-572ca2d4157f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
