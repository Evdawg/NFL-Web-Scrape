{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b8f006e-4d41-4da2-b738-edeb7bec7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: transfer nfl web scraping programs here. Clean and condense the code.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b003dadf-29aa-469e-860c-f00f55769713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arizona-cardinals', 'baltimore-ravens', 'atlanta-falcons', 'buffalo-bills', 'carolina-panthers', 'cincinnati-bengals', 'chicago-bears', 'cleveland-browns', 'dallas-cowboys', 'denver-broncos', 'detroit-lions', 'houston-texans', 'green-bay-packers', 'indianapolis-colts', 'los-angeles-rams', 'jacksonville-jaguars', 'minnesota-vikings', 'kansas-city-chiefs', 'new-orleans-saints', 'las-vegas-raiders', 'new-york-giants', 'los-angeles-chargers', 'philadelphia-eagles', 'miami-dolphins', 'san-francisco-49ers', 'new-england-patriots', 'seattle-seahawks', 'new-york-jets', 'tampa-bay-buccaneers', 'pittsburgh-steelers', 'washington-commanders', 'tennessee-titans']\n"
     ]
    }
   ],
   "source": [
    "team_list = ['arizona-cardinals',\n",
    "'baltimore-ravens',\n",
    "'atlanta-falcons',\n",
    "'buffalo-bills',\n",
    "'carolina-panthers',\n",
    "'cincinnati-bengals',\n",
    "'chicago-bears',\n",
    "'cleveland-browns',\n",
    "'dallas-cowboys',\n",
    "'denver-broncos',\n",
    "'detroit-lions',\n",
    "'houston-texans',\n",
    "'green-bay-packers',\n",
    "'indianapolis-colts',\n",
    "'los-angeles-rams',\n",
    "'jacksonville-jaguars',\n",
    "'minnesota-vikings',\n",
    "'kansas-city-chiefs',\n",
    "'new-orleans-saints',\n",
    "'las-vegas-raiders',\n",
    "'new-york-giants',\n",
    "'los-angeles-chargers',\n",
    "'philadelphia-eagles',\n",
    "'miami-dolphins',\n",
    "'san-francisco-49ers',\n",
    "'new-england-patriots',\n",
    "'seattle-seahawks',\n",
    "'new-york-jets',\n",
    "'tampa-bay-buccaneers',\n",
    "'pittsburgh-steelers',\n",
    "'washington-commanders',\n",
    "'tennessee-titans',]\n",
    "\n",
    "print(team_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "058b22c6-775d-4881-8239-38a4f9fee1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arizona-cardinals\n",
      "0/32\n",
      "Scraping roster table for https://www.nfl.com/teams/arizona-cardinals/roster/\n",
      "concatenating global roster_df with the arizona-cardinals DataFrame.\n",
      "Empty DataFrame\n",
      "Columns: [Player, No, Pos, Status, Height, Weight, Experience, College]\n",
      "Index: []\n",
      "baltimore-ravens\n",
      "1/32\n",
      "Scraping roster table for https://www.nfl.com/teams/baltimore-ravens/roster/\n",
      "concatenating global roster_df with the baltimore-ravens DataFrame.\n",
      "Empty DataFrame\n",
      "Columns: [Player, No, Pos, Status, Height, Weight, Experience, College]\n",
      "Index: []\n",
      "atlanta-falcons\n",
      "2/32\n",
      "Scraping roster table for https://www.nfl.com/teams/atlanta-falcons/roster/\n",
      "concatenating global roster_df with the atlanta-falcons DataFrame.\n",
      "Empty DataFrame\n",
      "Columns: [Player, No, Pos, Status, Height, Weight, Experience, College]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Player, No, Pos, Status, Height, Weight, Experience, College]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "### Send any Errors for specific team choice to a list:\n",
    "exceptions = []\n",
    "\n",
    "### Player links list is global to allow for appending this list in loop farther below.\n",
    "### levi says to write a function that has the dataframe as a parameter. Select statements if cannot concat straight up.\n",
    "Player_links = []\n",
    "\n",
    "roster_df = pd.DataFrame(columns = ['Player', 'No', 'Pos', 'Status', 'Height', 'Weight', 'Experience', 'College'])\n",
    "\n",
    "def data_concat():\n",
    "    print('concatenating global roster_df with the ' + team_choice + ' DataFrame.')\n",
    "    pd.concat([roster_df, df], ignore_index=True)\n",
    "\n",
    "\n",
    "### TODO: add a global DataFrame that each team roster df will be appended to. or something like that.\n",
    "\n",
    "### loop through NFL.com team rosters and pull out all player names, positions, etc.\n",
    "for i in range(0, 3):  #len(team_list)):\n",
    "    try:\n",
    "        team_choice = team_list[i]\n",
    "        print(team_choice)\n",
    "        print(str(i) + '/' + str(len(team_list)))\n",
    "\n",
    "        root = 'https://www.nfl.com/teams/'\n",
    "        url_end_piece = '/roster/'\n",
    "\n",
    "        url = root + team_choice + url_end_piece\n",
    "        print('Scraping roster table for '+ url)\n",
    "\n",
    "\n",
    "        ### create the soup object here\n",
    "        ### this is where you input the url and output the soup object that parses the webpage\n",
    "        result = requests.get(url)\n",
    "        webpage = result.content\n",
    "        soup = BeautifulSoup(webpage, \"html.parser\")\n",
    "\n",
    "        ### find the html table mark up and then find all row tag objects inside that table object\n",
    "        ### nfl.com stats pages only contain one table, so don't need to worry about processing multiple tables in this web scrape\n",
    "        table = soup.find('table', {'summary': 'Roster'})\n",
    "\n",
    "        headers = []\n",
    "        for i in table.find_all('th'):\n",
    "            title = i.text.strip()\n",
    "            headers.append(title)\n",
    "            #headers.append(['Player link'])\n",
    "\n",
    "        df = pd.DataFrame(columns = headers)\n",
    "        for row in table.findAll('tr')[1:]:\n",
    "            data = row.findAll('td')\n",
    "            row_data = [td.text.strip() for td in data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row_data\n",
    "        data_concat()\n",
    "        print(roster_df)\n",
    "            \n",
    "           \n",
    "        \n",
    "        ### TODO: \n",
    "        ### Working on building a second DF that will be merged with the NFL.com table scrape???\n",
    "        ### or try to add the additional column and row data in the loop above.\n",
    "        ### Need to add columns for the Player Name schema, Team, and Year.\n",
    "       \n",
    "        # for link in soup.findAll('a', {'class': 'nfl-o-roster__player-name nfl-o-cta--link'}):\n",
    "        #     Player_links.append(link.get('href'))\n",
    "        # print(Player_links)\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "### Try adding the player webpage link as a row item:      \n",
    "        # playerlink_df_headers = ['Player_Link', 'Team Name', 'Year']  \n",
    "        # playerlink_df = pd.DataFrame(columns = playerlink_df_headers)\n",
    "        # df['Player_link'] = str(soup.find('tr', {'nfl-o-roster__player-name nfl-o-cta--link'}))\n",
    "        # df['Team Name'] = str(soup.find('div', {'nfl-c-team-header__title'}).text.strip())\n",
    "        # df['Year'] = date.today().year\n",
    "        # print(playerlink_df)\n",
    "             \n",
    "### Then append the dataframes\n",
    "      \n",
    "            \n",
    "    except AttributeError:    # the except statement sends Attribute errors to a list that is exported after the loop is finished\n",
    "        exceptions.append(team_choice)\n",
    "\n",
    "    with open(r'C:\\Users\\EvanS\\Programming\\PyCharm\\Projects\\NFL-Web-Scrape-V2\\Scrap files\\review_roster_scrape_output.csv', 'w') as fp:\n",
    "        fp.write('\\n'.join(exceptions))\n",
    "        \n",
    "### Manually put together second dataframe with only the Player link field:\n",
    "# Player_link_df  = pd.DataFrame(data = Player_links, columns = ['Player link'])\n",
    "# print(Player_link_df)\n",
    "\n",
    "\n",
    "print(roster_df)\n",
    "\n",
    "df.to_csv(r'C:\\Users\\EvanS\\Programming\\PyCharm\\Projects\\NFL-Web-Scrape-V2\\Scrap files\\test.csv')\n",
    "\n",
    "### The dataframe is being overwritten with each loop pass. Need to fix it so all pages scraped go to the same dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4093448-4443-4338-9c60-a3c138dac1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\EvanS\\\\Programming\\\\PyCharm\\\\Projects\\\\NFL-Web-Scrape-V2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50431da7-ccc4-4ee3-a6a8-cde433c5d8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8e116-2ed1-47f0-849f-1f215b547b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce4d4b8-8303-46f7-bddd-572ca2d4157f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0531c4-140b-4d23-ae60-ec97edb6246f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f86ffc-0215-4a9e-9dc7-1aad3819ad6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a20db6-ef87-47b9-8031-241b2fa90b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416e5f3-a2f5-4144-8ca8-a094437ac521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0014232-2891-422b-b0cc-959994f0698a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218f735-7628-498b-aa77-a241b22b94c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e1a72-2e8a-400d-b321-27f9db677f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed76564-951d-46c4-9ff0-97db3d352b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5808d-e6e8-450a-b015-05588f07a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd8f77-9754-4847-9c45-a1b6c8f80c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
